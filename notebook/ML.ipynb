{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2679f704",
   "metadata": {},
   "source": [
    "# An√°lisis Comparativo de Modelos Ensemble para Predicci√≥n de Consumo Energ√©tico\n",
    " \n",
    "Este notebook implementa y compara diferentes algoritmos de ensemble learning para la predicci√≥n del consumo energ√©tico, incluyendo Random Forest, Gradient Boosting y XGBoost. El objetivo es identificar el mejor modelo y m√©todo de optimizaci√≥n de hiperpar√°metros.\n",
    " \n",
    "## Estructura del An√°lisis\n",
    "1. Carga y Preparaci√≥n de Datos: Dataset preprocesado con adici√≥n de ruido realista\n",
    "2. Modelos Base: Implementaci√≥n de tres algoritmos ensemble principales\n",
    "3. Optimizaci√≥n de Hiperpar√°metros: Comparaci√≥n de GridSearch, RandomSearch y BayesianSearch\n",
    "4. Evaluaci√≥n Comparativa: An√°lisis de rendimiento entre todos los enfoques\n",
    "5. Visualizaciones: Gr√°ficos diagn√≥sticos y comparativos\n",
    "6. Persistencia: Guardado del mejor modelo para producci√≥n\n",
    " \n",
    "### Configuraci√≥n del Entorno\n",
    " \n",
    "Librer√≠as clave para ensemble learning:\n",
    "- sklearn.ensemble: Random Forest y Gradient Boosting cl√°sicos\n",
    "- xgboost: Implementaci√≥n optimizada de Gradient Boosting\n",
    "- skopt: Optimizaci√≥n bayesiana de hiperpar√°metros\n",
    " \n",
    "¬øPor qu√© modelos ensemble?\n",
    "Los m√©todos ensemble combinan m√∫ltiples modelos para obtener mejor rendimiento que cualquier modelo individual:\n",
    "- Reducci√≥n de varianza: Promedio de m√∫ltiples predicciones reduce ruido\n",
    "- Reducci√≥n de sesgo: Diferentes modelos capturan diferentes patrones\n",
    "- Mayor robustez: Menos sensibles a outliers y ruido en datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ce554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f81cd4",
   "metadata": {},
   "source": [
    "### Carga de Datasets Preprocesados\n",
    " \n",
    "Utilizamos los mismos datasets que en el an√°lisis SVR para asegurar comparabilidad directa entre diferentes enfoques de modelado. Los datos ya han pasado por:\n",
    "- Codificaci√≥n de variables categ√≥ricas (one-hot encoding)\n",
    "- Limpieza y tratamiento de valores faltantes\n",
    "- Estructuraci√≥n consistente entre train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b128ab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARGAR Y PREPARAR LOS DATOS\n",
    "train_df = pd.read_csv('../data/processed/energy_data_processed.csv')\n",
    "test_df = pd.read_csv('../data/processed/energy_data_processed_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bbc4aa",
   "metadata": {},
   "source": [
    "### Separaci√≥n de Variables y Adici√≥n de Variabilidad Realista\n",
    " \n",
    "Separaci√≥n est√°ndar:\n",
    "- X_train, X_test: Matrices de caracter√≠sticas para entrenamiento y prueba\n",
    "- y_train, y_test: Vectores de variable objetivo\n",
    " \n",
    "Adici√≥n de ruido gaussiano:\n",
    "Se mantiene la misma estrategia del notebook SVR para consistencia:\n",
    "- noise_factor = 0.1: 10% del rango total de la variable objetivo\n",
    "- Distribuci√≥n normal: Ruido m√°s realista que ruido uniforme\n",
    "- Mismo seed: Garantiza reproducibilidad entre experimentos\n",
    " \n",
    "Justificaci√≥n del ruido:\n",
    "- Datos perfectos son irreales: En la pr√°ctica siempre hay variabilidad no explicada\n",
    "- Evita overfitting artificial: Modelos demasiado perfectos no generalizan bien\n",
    "- Simula incertidumbre: Refleja errores de medici√≥n y factores no observados\n",
    " \n",
    "Monitoreo de la transformaci√≥n:\n",
    "- Verificamos que el rango y varianza del ruido sean apropiados\n",
    "- Mantenemos las propiedades estad√≠sticas b√°sicas de los datos originales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162aa5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset de entrenamiento: (1000, 9)\n",
      "Forma del dataset de prueba: (100, 9)\n",
      "Variable objetivo: Energy Consumption\n",
      "\n",
      "Varianza de y_train con ruido: 1086917.99596434\n",
      "Rango de y_train con ruido: [1442.89, 6860.07]\n",
      "Varianza de y_test con ruido: 973255.7143404601\n",
      "Rango de y_test con ruido: [1875.14, 6906.85]\n"
     ]
    }
   ],
   "source": [
    "# 2. SEPARAR CARACTER√çSTICAS Y VARIABLE OBJETIVO\n",
    "y_train = train_df['Energy Consumption']\n",
    "X_train = train_df.drop('Energy Consumption', axis=1)\n",
    "y_test = test_df['Energy Consumption']\n",
    "X_test = test_df.drop('Energy Consumption', axis=1)\n",
    "\n",
    "# Verificar tama√±os\n",
    "print(f'Forma del dataset de entrenamiento: {X_train.shape}')\n",
    "print(f'Forma del dataset de prueba: {X_test.shape}')\n",
    "print(f'Variable objetivo: {y_train.name}')\n",
    "\n",
    "# A√±adir ruido aleatorio para simular datos m√°s realistas\n",
    "np.random.seed(42)  # Para reproducibilidad\n",
    "noise_factor = 0.1  # 10% del rango de y_train\n",
    "y_train_range = y_train.max() - y_train.min()\n",
    "y_train_noisy = y_train + np.random.normal(0, noise_factor * y_train_range, size=y_train.shape)\n",
    "y_test_noisy = y_test + np.random.normal(0, noise_factor * y_train_range, size=y_test.shape)\n",
    "print(\"\\nVarianza de y_train con ruido:\", y_train_noisy.var())\n",
    "print(\"Rango de y_train con ruido: [{:.2f}, {:.2f}]\".format(y_train_noisy.min(), y_train_noisy.max()))\n",
    "print(\"Varianza de y_test con ruido:\", y_test_noisy.var())\n",
    "print(\"Rango de y_test con ruido: [{:.2f}, {:.2f}]\".format(y_test_noisy.min(), y_test_noisy.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c634b",
   "metadata": {},
   "source": [
    "### Escalado de Caracter√≠sticas para Modelos Ensemble\n",
    " \n",
    "¬øEs necesario escalar para √°rboles de decisi√≥n?\n",
    "- Random Forest/Gradient Boosting: No requieren escalado (son invariantes a transformaciones mon√≥tonas)\n",
    "- XGBoost: Tampoco requiere escalado estrictamente\n",
    " \n",
    "¬øPor qu√© escalamos entonces?\n",
    "1. Consistencia experimental: Mantener las mismas condiciones que en SVR\n",
    "2. Comparabilidad: Eliminamos el escalado como variable confundidora\n",
    "3. Futuros experimentos: Si queremos probar modelos h√≠bridos o ensemble con algoritmos sensibles al escalado\n",
    "4. Regularizaci√≥n: Algunos par√°metros de regularizaci√≥n pueden funcionar mejor con datos escalados\n",
    " \n",
    "Proceso de escalado:\n",
    "- fit_transform en entrenamiento: Calcula media y desviaci√≥n est√°ndar\n",
    "- transform en prueba: Aplica la misma transformaci√≥n sin recalcular estad√≠sticas\n",
    "- Prevenci√≥n de data leakage: No usamos informaci√≥n de test para el escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb19c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ESCALADO DE CARACTER√çSTICAS (opcional para √°rboles, pero √∫til para consistencia)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e78a0",
   "metadata": {},
   "source": [
    "### Implementaci√≥n de Modelos Ensemble Base\n",
    " \n",
    "Configuraci√≥n de cada algoritmo:\n",
    " \n",
    "1. Random Forest:\n",
    "- n_estimators=100: 100 √°rboles para balance entre rendimiento y velocidad\n",
    "- max_depth=10: Limita profundidad para controlar overfitting\n",
    "- min_samples_split=5: M√≠nimo 5 muestras para dividir un nodo\n",
    "- min_samples_leaf=2: M√≠nimo 2 muestras en hojas terminales\n",
    "- Estrategia: Bagging (bootstrap + promedio) para reducir varianza\n",
    " \n",
    "2. Gradient Boosting:\n",
    "- n_estimators=100: 100 iteraciones de boosting\n",
    "- max_depth=5: √Årboles m√°s simples (stumps mejorados)\n",
    "- Mismos par√°metros de regularizaci√≥n que Random Forest\n",
    "- Estrategia: Boosting secuencial que corrige errores previos\n",
    " \n",
    "3. XGBoost:\n",
    "- n_estimators=50: Menos iteraciones (XGBoost es m√°s eficiente)\n",
    "- max_depth=3: √Årboles muy simples para evitar overfitting\n",
    "- reg_alpha=1.0, reg_lambda=2.0: Regularizaci√≥n L1 y L2\n",
    "- gamma=0.5: Penalizaci√≥n por complejidad del √°rbol\n",
    "- Estrategia: Gradient boosting optimizado con regularizaci√≥n avanzada\n",
    " \n",
    "Diferencias clave entre algoritmos:\n",
    "- Random Forest: Paralelo, robusto, menos prone a overfitting\n",
    "- Gradient Boosting: Secuencial, m√°s expressivo, puede overfittear\n",
    "- XGBoost: Gradient boosting optimizado, mejor regularizaci√≥n, m√°s r√°pido\n",
    " \n",
    "Evaluaci√≥n inmediata:\n",
    "Calculamos m√©tricas base para establecer benchmark antes de optimizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0011545d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest B√ÅSICO ===\n",
      "MSE: 232561.3204\n",
      "RMSE: 482.2461\n",
      "R¬≤: 0.7586\n",
      "MAE: 388.5226\n",
      "\n",
      "=== Gradient Boosting B√ÅSICO ===\n",
      "MSE: 252435.1228\n",
      "RMSE: 502.4292\n",
      "R¬≤: 0.7380\n",
      "MAE: 397.0668\n",
      "\n",
      "=== XGBoost B√ÅSICO ===\n",
      "MSE: 265444.2876\n",
      "RMSE: 515.2129\n",
      "R¬≤: 0.7245\n",
      "MAE: 412.0864\n"
     ]
    }
   ],
   "source": [
    "# 4. MODELOS ENSEMBLE\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=5, min_samples_leaf=2),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5, min_samples_split=5, min_samples_leaf=2),\n",
    "    'XGBoost': XGBRegressor(n_estimators=50, random_state=42, eval_metric='rmse', max_depth=3, reg_alpha=1.0, reg_lambda=2.0, gamma=0.5)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar modelos b√°sicos\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f'\\n=== {name} B√ÅSICO ===')\n",
    "    model.fit(X_train_scaled, y_train_noisy)  # Usar y_train_noisy\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test_noisy, y_pred_test)  # Usar y_test_noisy\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_noisy, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test_noisy, y_pred_test)\n",
    "    \n",
    "    results[name] = {'MSE': mse, 'RMSE': rmse, 'R¬≤': r2, 'MAE': mae}\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bb198",
   "metadata": {},
   "source": [
    "### An√°lisis Diagn√≥stico Preliminar\n",
    " \n",
    "An√°lisis de correlaciones:\n",
    "- Identifica qu√© caracter√≠sticas tienen mayor relaci√≥n lineal con la variable objetivo\n",
    "- Ordenamiento descendente muestra las caracter√≠sticas m√°s predictivas\n",
    "- Interpretaci√≥n: Valores |r| > 0.5 son correlaciones moderadas-fuertes\n",
    " \n",
    "Validaci√≥n cruzada de XGBoost:\n",
    "- ¬øPor qu√© solo XGBoost?: Es t√≠picamente el algoritmo de mayor rendimiento\n",
    "- 5-fold CV: Proporciona estimaci√≥n robusta del rendimiento esperado\n",
    "- Scoring='r2': M√©trica de varianza explicada, f√°cil de interpretar\n",
    "- Intervalos de confianza: ¬±2 desviaciones est√°ndar (aprox. 95% confianza)\n",
    " \n",
    "Prop√≥sito del diagn√≥stico:\n",
    "- Baseline establecido: Rendimiento antes de optimizaci√≥n\n",
    "- Identificaci√≥n de caracter√≠sticas: Cu√°les son m√°s importantes\n",
    "- Estimaci√≥n de estabilidad: Qu√© tan consistente es el modelo\n",
    "\n",
    "Uso de datos con ruido:\n",
    "- Todas las evaluaciones usan y_train_noisy y y_test_noisy\n",
    "- Mantiene consistencia con la estrategia de realismo de datos\n",
    "- Permite comparaci√≥n directa con el notebook SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa982d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DIAGN√ìSTICO ADICIONAL ===\n",
      "\n",
      "Correlaciones entre caracter√≠sticas y 'Energy Consumption' (con ruido):\n",
      "Square Footage               0.694921\n",
      "Building Type_Industrial     0.369221\n",
      "Number of Occupants          0.305407\n",
      "Appliances Used              0.283204\n",
      "Day of Week_Weekday          0.018533\n",
      "Average Temperature         -0.014925\n",
      "Day of Week_Weekend         -0.018533\n",
      "Building Type_Commercial    -0.030466\n",
      "Building Type_Residential   -0.330685\n",
      "dtype: float64\n",
      "\n",
      "Validaci√≥n cruzada (5-fold) para XGBoost - R¬≤: 0.7505 (+/- 0.0595)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DIAGN√ìSTICO ADICIONAL ===\")\n",
    "\n",
    "# Correlaciones entre caracter√≠sticas y variable objetivo\n",
    "correlations = X_train.corrwith(y_train_noisy)  # Usar y_train_noisy\n",
    "print(\"\\nCorrelaciones entre caracter√≠sticas y 'Energy Consumption' (con ruido):\")\n",
    "print(correlations.sort_values(ascending=False))\n",
    "\n",
    "# Validaci√≥n cruzada para XGBoost b√°sico\n",
    "cv_scores = cross_val_score(models['XGBoost'], X_train_scaled, y_train_noisy, cv=5, scoring='r2')  # Usar y_train_noisy\n",
    "print(f\"\\nValidaci√≥n cruzada (5-fold) para XGBoost - R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f7aa2c",
   "metadata": {},
   "source": [
    "### Comparaci√≥n Exhaustiva de M√©todos de Optimizaci√≥n de Hiperpar√°metros\n",
    " \n",
    "Estrategia experimental:\n",
    "Se implementan tres enfoques diferentes para optimizar XGBoost y determinar cu√°l es m√°s efectivo para este problema espec√≠fico.\n",
    " \n",
    "1. GridSearchCV - B√∫squeda Exhaustiva:\n",
    "- Ventajas: Garantiza encontrar el √≥ptimo dentro del espacio definido\n",
    "- Limitaciones: Computacionalmente costoso, crece exponencialmente\n",
    "- Espacio reducido: 2√ó2√ó2√ó2√ó2√ó2√ó2 = 128 combinaciones\n",
    "- Uso t√≠pico: Espacios peque√±os, cuando hay tiempo suficiente\n",
    " \n",
    "2. RandomizedSearchCV - B√∫squeda Aleatoria:\n",
    "- Ventajas: M√°s eficiente, explora mejor espacios de alta dimensionalidad\n",
    "- n_iter=50: Eval√∫a 50 combinaciones aleatorias\n",
    "- Espacio completo: Usa todas las opciones del param_grid\n",
    "- Uso t√≠pico: Exploraci√≥n inicial, espacios grandes\n",
    " \n",
    "3. BayesSearchCV - Optimizaci√≥n Bayesiana:\n",
    "- Ventajas: M√°s inteligente, usa informaci√≥n de evaluaciones previas\n",
    "- Espacios continuos: Real() permite explorar valores intermedios\n",
    "- Prior='log-uniform': Para learning_rate, explora escalas logar√≠tmicas\n",
    "- Uso t√≠pico: Problemas complejos, presupuesto limitado de evaluaciones\n",
    " \n",
    "Definici√≥n de espacios de b√∫squeda:\n",
    "\n",
    "Par√°metros clave de XGBoost:\n",
    "- n_estimators: N√∫mero de √°rboles (m√°s = mejor ajuste, m√°s overfitting)\n",
    "- max_depth: Profundidad m√°xima (m√°s = m√°s complejo, m√°s overfitting)\n",
    "- learning_rate: Tasa de aprendizaje (menor = m√°s conservador, necesita m√°s √°rboles)\n",
    "- subsample: Fracci√≥n de muestras por √°rbol (< 1.0 = regularizaci√≥n)\n",
    "- colsample_bytree: Fracci√≥n de caracter√≠sticas por √°rbol (regularizaci√≥n)\n",
    "- reg_alpha: Regularizaci√≥n L1 (sparsity, selecci√≥n de caracter√≠sticas)\n",
    "- reg_lambda: Regularizaci√≥n L2 (suavidad, previene overfitting)\n",
    "- gamma: Penalizaci√≥n m√≠nima para divisi√≥n (m√°s = m√°s conservador)\n",
    " \n",
    "Estrategia de evaluaci√≥n:\n",
    "- 5-fold CV: Balance entre confiabilidad y costo computacional\n",
    "- neg_mean_squared_error: Optimiza directamente la m√©trica objetivo\n",
    "- n_jobs=-1: Paralelizaci√≥n para acelerar b√∫squeda\n",
    " \n",
    "Selecci√≥n autom√°tica del mejor m√©todo:\n",
    "- Compara MSE en conjunto de prueba\n",
    "- Selecciona autom√°ticamente el modelo con menor error\n",
    "- Proporciona trazabilidad completa del proceso de selecci√≥n\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1e865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GridSearchCV ---\n",
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n",
      "Mejores par√°metros (GridSearchCV): {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'reg_alpha': 0.0, 'reg_lambda': 1.0, 'subsample': 0.8}\n",
      "Mejor MSE CV (GridSearchCV): 243796.0891\n",
      "\n",
      "--- RandomizedSearchCV ---\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "Mejores par√°metros (RandomizedSearchCV): {'subsample': 0.6, 'reg_lambda': 1.0, 'reg_alpha': 0.0, 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.3, 'colsample_bytree': 0.6}\n",
      "Mejor MSE CV (RandomizedSearchCV): 249594.2863\n",
      "\n",
      "--- BayesSearchCV ---\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Mejores par√°metros (BayesSearchCV): OrderedDict({'colsample_bytree': 0.6, 'gamma': 0.5, 'learning_rate': 0.10023856637823951, 'max_depth': 3, 'n_estimators': 122, 'reg_alpha': 1.0, 'reg_lambda': 0.9773104393514005, 'subsample': 0.6})\n",
      "Mejor MSE CV (BayesSearchCV): 245527.5342\n",
      "\n",
      "=== COMPARACI√ìN DE M√âTODOS DE OPTIMIZACI√ìN ===\n",
      "\n",
      "GridSearchCV (Prueba):\n",
      "MSE: 238829.9856\n",
      "RMSE: 488.7023\n",
      "R¬≤: 0.7521\n",
      "MAE: 393.0759\n",
      "\n",
      "RandomizedSearchCV (Prueba):\n",
      "MSE: 242187.9053\n",
      "RMSE: 492.1259\n",
      "R¬≤: 0.7486\n",
      "MAE: 390.9323\n",
      "\n",
      "BayesSearchCV (Prueba):\n",
      "MSE: 256503.1255\n",
      "RMSE: 506.4614\n",
      "R¬≤: 0.7338\n",
      "MAE: 404.6343\n",
      "\n",
      "Mejor m√©todo: GridSearchCV (MSE: 238829.9856)\n"
     ]
    }
   ],
   "source": [
    "# 5. OPTIMIZACI√ìN CON BAYESIAN SEARCH PARA XGBoost (ejemplo, puedes extender a otros)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Definir espacio de hiperpar√°metros com√∫n\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.5, 1.0],\n",
    "    'reg_lambda': [0.0, 1.0, 2.0],\n",
    "    'gamma': [0.0, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# 1. GridSearchCV\n",
    "print(\"\\n--- GridSearchCV ---\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0.0, 1.0],\n",
    "        'reg_lambda': [1.0, 2.0]\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train_noisy)  # Usar y_train_noisy\n",
    "grid_best = grid_search.best_estimator_\n",
    "grid_score = -grid_search.best_score_\n",
    "print(f\"Mejores par√°metros (GridSearchCV): {grid_search.best_params_}\")\n",
    "print(f\"Mejor MSE CV (GridSearchCV): {grid_score:.4f}\")\n",
    "\n",
    "# 2. RandomizedSearchCV\n",
    "print(\"\\n--- RandomizedSearchCV ---\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train_scaled, y_train_noisy)  # Usar y_train_noisy\n",
    "random_best = random_search.best_estimator_\n",
    "random_score = -random_search.best_score_\n",
    "print(f\"Mejores par√°metros (RandomizedSearchCV): {random_search.best_params_}\")\n",
    "print(f\"Mejor MSE CV (RandomizedSearchCV): {random_score:.4f}\")\n",
    "\n",
    "# 3. BayesSearchCV\n",
    "print(\"\\n--- BayesSearchCV ---\")\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    search_spaces={\n",
    "        'n_estimators': Integer(50, 200),\n",
    "        'max_depth': Integer(3, 8),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0),\n",
    "        'reg_alpha': Real(0.0, 1.0),\n",
    "        'reg_lambda': Real(0.0, 2.0),\n",
    "        'gamma': Real(0.0, 0.5)\n",
    "    },\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "bayes_search.fit(X_train_scaled, y_train_noisy)  # Usar y_train_noisy y todo el conjunto de entrenamiento\n",
    "bayes_best = bayes_search.best_estimator_\n",
    "bayes_score = -bayes_search.best_score_\n",
    "print(f\"Mejores par√°metros (BayesSearchCV): {bayes_search.best_params_}\")\n",
    "print(f\"Mejor MSE CV (BayesSearchCV): {bayes_score:.4f}\")\n",
    "\n",
    "# No reentrenar con early stopping debido a incompatibilidad; usar bayes_best directamente\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\n=== COMPARACI√ìN DE M√âTODOS DE OPTIMIZACI√ìN ===\")\n",
    "opt_results = {}\n",
    "for name, model in [('GridSearchCV', grid_best), ('RandomizedSearchCV', random_best), ('BayesSearchCV', bayes_best)]:\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test_noisy, y_pred_test)  # Usar y_test_noisy\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test_noisy, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test_noisy, y_pred_test)\n",
    "    opt_results[name] = {'MSE': mse, 'RMSE': rmse, 'R¬≤': r2, 'MAE': mae}\n",
    "    print(f\"\\n{name} (Prueba):\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "best_method = min(opt_results, key=lambda x: opt_results[x]['MSE'])\n",
    "ensemble_optimized = {'GridSearchCV': grid_best, 'RandomizedSearchCV': random_best, 'BayesSearchCV': bayes_best}[best_method]\n",
    "print(f\"\\nMejor m√©todo: {best_method} (MSE: {opt_results[best_method]['MSE']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86892f50",
   "metadata": {},
   "source": [
    "### Evaluaci√≥n Completa del Modelo Ensemble Optimizado\n",
    " \n",
    "An√°lisis del mejor modelo seleccionado:\n",
    "El modelo ensemble_optimized corresponde al algoritmo que obtuvo el menor MSE entre los tres m√©todos de optimizaci√≥n probados.\n",
    " \n",
    "Evaluaci√≥n en ambos conjuntos:\n",
    " \n",
    "Conjunto de entrenamiento:\n",
    "- Prop√≥sito: Verificar capacidad de ajuste del modelo\n",
    "- Interpretaci√≥n: Qu√© tan bien \"recuerda\" los datos de entrenamiento\n",
    "- Se√±al de alerta: Si es demasiado perfecto (R¬≤ ‚âà 1), posible overfitting\n",
    " \n",
    "Conjunto de prueba:\n",
    "- M√°s importante: Mide capacidad real de generalizaci√≥n\n",
    "- Interpretaci√≥n: Rendimiento esperado en datos nuevos\n",
    "- M√©trica cr√≠tica: Esta es la que realmente importa para producci√≥n\n",
    " \n",
    "Detecci√≥n de overfitting:\n",
    "- Gap train-test peque√±o: Modelo bien balanceado\n",
    "- Gap train-test grande: Posible overfitting\n",
    "- Test mejor que train: Posible underfitting (raro) o casualidad estad√≠stica\n",
    " \n",
    "Validaci√≥n cruzada adicional:\n",
    "- ¬øPor qu√© otra CV?: Confirmaci√≥n independiente del rendimiento\n",
    "- Datos originales: Usa y_train sin ruido para comparaci√≥n\n",
    "- Intervalos de confianza: Proporciona rango de rendimiento esperado\n",
    " \n",
    "M√©tricas complementarias:\n",
    "- MSE: Penaliza errores grandes, √∫til para optimizaci√≥n\n",
    "- RMSE: Mismas unidades que la variable objetivo, interpretable\n",
    "- R¬≤: Porcentaje de varianza explicada, f√°cil de comunicar\n",
    "- MAE: Robusto a outliers, error promedio t√≠pico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03ac1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODELO OPTIMIZADO (ENTRENAMIENTO) ===\n",
      "MSE: 152717.0443\n",
      "RMSE: 390.7903\n",
      "R¬≤: 0.8594\n",
      "MAE: 312.1552\n",
      "\n",
      "=== MODELO OPTIMIZADO (PRUEBA) ===\n",
      "MSE: 238829.9856\n",
      "RMSE: 488.7023\n",
      "R¬≤: 0.7521\n",
      "MAE: 393.0759\n",
      "\n",
      "CV RMSE: 80.5791 (+/- 33.7801)\n"
     ]
    }
   ],
   "source": [
    "# 6. MODELO OPTIMIZADO (usamos XGBoost optimizado como ejemplo principal)\n",
    "y_pred_train_opt = ensemble_optimized.predict(X_train_scaled)\n",
    "y_pred_test_opt = ensemble_optimized.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas para entrenamiento\n",
    "mse_train_opt = mean_squared_error(y_train_noisy, y_pred_train_opt)  # Usar y_train_noisy\n",
    "rmse_train_opt = np.sqrt(mse_train_opt)\n",
    "r2_train_opt = r2_score(y_train_noisy, y_pred_train_opt)\n",
    "mae_train_opt = mean_absolute_error(y_train_noisy, y_pred_train_opt)\n",
    "\n",
    "# M√©tricas para prueba\n",
    "mse_test_opt = mean_squared_error(y_test_noisy, y_pred_test_opt)  # Usar y_test_noisy\n",
    "rmse_test_opt = np.sqrt(mse_test_opt)\n",
    "r2_test_opt = r2_score(y_test_noisy, y_pred_test_opt)\n",
    "mae_test_opt = mean_absolute_error(y_test_noisy, y_pred_test_opt)\n",
    "\n",
    "print(f'\\n=== MODELO OPTIMIZADO (ENTRENAMIENTO) ===')\n",
    "print(f'MSE: {mse_train_opt:.4f}')\n",
    "print(f'RMSE: {rmse_train_opt:.4f}')\n",
    "print(f'R¬≤: {r2_train_opt:.4f}')\n",
    "print(f'MAE: {mae_train_opt:.4f}')\n",
    "\n",
    "print(f'\\n=== MODELO OPTIMIZADO (PRUEBA) ===')\n",
    "print(f'MSE: {mse_test_opt:.4f}')\n",
    "print(f'RMSE: {rmse_test_opt:.4f}')\n",
    "print(f'R¬≤: {r2_test_opt:.4f}')\n",
    "print(f'MAE: {mae_test_opt:.4f}')\n",
    "\n",
    "# Validaci√≥n cruzada\n",
    "cv_scores = cross_val_score(ensemble_optimized, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'\\nCV RMSE: {np.sqrt(-cv_scores.mean()):.4f} (+/- {np.sqrt(cv_scores.std() * 2):.4f})')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7aeba",
   "metadata": {},
   "source": [
    "### Persistencia de Predicciones para An√°lisis Detallado\n",
    " \n",
    "Estructura de archivos generados:\n",
    " \n",
    "Columnas incluidas:\n",
    "- Valores Reales: Variable objetivo con ruido (y_train_noisy, y_test_noisy)\n",
    "- Predicciones: Salida del modelo ensemble optimizado\n",
    "- Diferencia: Error residual (real - predicho)\n",
    " \n",
    "Utilidades de estos archivos:\n",
    " \n",
    "An√°lisis post-hoc:\n",
    "- Identificaci√≥n de outliers: Casos con errores muy grandes\n",
    "- An√°lisis de patrones: ¬øEn qu√© rangos el modelo falla m√°s?\n",
    "- Distribuci√≥n de errores: ¬øSon normalmente distribuidos?\n",
    " \n",
    "Comparaci√≥n entre modelos:\n",
    "- Consistencia: Comparar con predicciones de SVR\n",
    "- Benchmarking: Usar como baseline para futuros modelos\n",
    "- Ensemble de modelos: Combinar predicciones de diferentes enfoques\n",
    " \n",
    "Validaci√≥n de negocio:\n",
    "- Casos cr√≠ticos: Identificar predicciones problem√°ticas\n",
    "- Intervalos de confianza: Estimar incertidumbre de predicciones\n",
    "- An√°lisis de sensibilidad: ¬øQu√© caracter√≠sticas afectan m√°s las diferencias?\n",
    "\n",
    "Uso de datos con ruido:\n",
    "- Mantiene consistencia con la estrategia experimental\n",
    "- Permite an√°lisis comparativo directo con otros notebooks\n",
    "- Refleja incertidumbre realista en los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3762c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicciones de entrenamiento guardadas en '../data/results/ensemble_predictions_train.csv'\n",
      "Predicciones de prueba guardadas en '../data/results/ensemble_predictions_test.csv'\n"
     ]
    }
   ],
   "source": [
    "# 7. GUARDAR RESULTADOS EN CSV\n",
    "train_results = pd.DataFrame({\n",
    "    'Valores Reales': y_train_noisy,  # Usar y_train_noisy\n",
    "    'Predicciones': y_pred_train_opt,\n",
    "    'Diferencia': y_train_noisy - y_pred_train_opt\n",
    "})\n",
    "train_results.to_csv('../data/results/ensemble_predictions_train.csv', index=False)\n",
    "print(\"\\nPredicciones de entrenamiento guardadas en '../data/results/ensemble_predictions_train.csv'\")\n",
    "\n",
    "test_results = pd.DataFrame({\n",
    "    'Valores Reales': y_test_noisy,  # Usar y_test_noisy\n",
    "    'Predicciones': y_pred_test_opt,\n",
    "    'Diferencia': y_test_noisy - y_pred_test_opt\n",
    "})\n",
    "test_results.to_csv('../data/results/ensemble_predictions_test.csv', index=False)\n",
    "print(\"Predicciones de prueba guardadas en '../data/results/ensemble_predictions_test.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101bdc6",
   "metadata": {},
   "source": [
    "### Panel Completo de Visualizaciones Diagn√≥sticas\n",
    " \n",
    "Estrategia de visualizaci√≥n en dos paneles:\n",
    " \n",
    "PANEL 1 - Conjunto de Prueba (2√ó2):\n",
    "El m√°s cr√≠tico porque eval√∫a la capacidad de generalizaci√≥n real\n",
    " \n",
    "Gr√°fico Superior Izquierda - Predicciones vs Valores Reales:\n",
    "- L√≠nea diagonal roja: Referencia de predicci√≥n perfecta (y = x)\n",
    "- Dispersi√≥n de puntos: Indica variabilidad de errores\n",
    "- Concentraci√≥n en diagonal: Se√±al de buenas predicciones\n",
    "- Patrones sistem√°ticos: Podr√≠an indicar bias del modelo\n",
    " \n",
    "Gr√°fico Superior Derecha - An√°lisis de Residuos:\n",
    "- L√≠nea horizontal en y=0: Referencia de error cero\n",
    "- Distribuci√≥n aleatoria: Indicador de modelo bien especificado\n",
    "- Patrones o tendencias: Se√±ales de problemas (heterocedasticidad, no linealidad)\n",
    "- Outliers extremos: Casos que el modelo predice mal\n",
    " \n",
    "Gr√°ficos Inferiores - Comparaci√≥n de Modelos:\n",
    "- MSE (Izquierda): Comparaci√≥n de errores absolutos\n",
    "- R¬≤ (Derecha): Comparaci√≥n de capacidad explicativa\n",
    "- 6 modelos totales: 3 b√°sicos + 3 m√©todos de optimizaci√≥n\n",
    "- Rotaci√≥n de etiquetas: Evita solapamiento de nombres\n",
    " \n",
    "PANEL 2 - Conjunto de Entrenamiento (1√ó2):\n",
    " Complementa el an√°lisis para detectar overfitting\n",
    " \n",
    "Interpretaci√≥n conjunta Train vs Test:\n",
    "- Similaridad: Modelo bien balanceado\n",
    "- Train perfecto, Test imperfecto: Overfitting\n",
    "- Ambos imperfectos pero similares: Underfitting controlado\n",
    " \n",
    "Detalles t√©cnicos:\n",
    "- alpha=0.6: Transparencia para ver densidad de puntos\n",
    "- dpi=300: Alta resoluci√≥n para publicaci√≥n\n",
    "- bbox_inches='tight': Elimina espacios innecesarios\n",
    "- plt.close(): Libera memoria despu√©s de guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dfa775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Gr√°fico de prueba guardado en: ../data/figures/ensemble_comparative_analysis_test.png\n",
      "üìä Gr√°fico de entrenamiento guardado en: ../data/figures/ensemble_comparative_analysis_train.png\n"
     ]
    }
   ],
   "source": [
    "# 8. VISUALIZACIONES\n",
    "output_dir = \"../data/figures/\"\n",
    "\n",
    "# Visualizaciones para el conjunto de prueba\n",
    "fig_test, axes_test = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes_test[0,0].scatter(y_test_noisy, y_pred_test_opt, alpha=0.6)  # Usar y_test_noisy\n",
    "axes_test[0,0].plot([y_test_noisy.min(), y_test_noisy.max()], [y_test_noisy.min(), y_test_noisy.max()], 'r--', lw=2)\n",
    "axes_test[0,0].set_xlabel('Valores Reales')\n",
    "axes_test[0,0].set_ylabel('Predicciones')\n",
    "axes_test[0,0].set_title(f'Ensemble Optimizado (Prueba) - R¬≤ = {r2_test_opt:.4f}')\n",
    "axes_test[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "residuos_test = y_test_noisy - y_pred_test_opt  # Usar y_test_noisy\n",
    "axes_test[0,1].scatter(y_pred_test_opt, residuos_test, alpha=0.6)\n",
    "axes_test[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes_test[0,1].set_xlabel('Predicciones')\n",
    "axes_test[0,1].set_ylabel('Residuos')\n",
    "axes_test[0,1].set_title('Gr√°fico de Residuos (Prueba)')\n",
    "axes_test[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "model_names = list(results.keys()) + ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']\n",
    "mse_values = [results[m]['MSE'] for m in results] + [opt_results[m]['MSE'] for m in ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']]\n",
    "r2_values = [results[m]['R¬≤'] for m in results] + [opt_results[m]['R¬≤'] for m in ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "axes_test[1,0].bar(x_pos, mse_values, alpha=0.7)\n",
    "axes_test[1,0].set_xlabel('Modelo')\n",
    "axes_test[1,0].set_ylabel('MSE')\n",
    "axes_test[1,0].set_title('Comparaci√≥n MSE por Modelo')\n",
    "axes_test[1,0].set_xticks(x_pos)\n",
    "axes_test[1,0].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "axes_test[1,1].bar(x_pos, r2_values, alpha=0.7, color='green')\n",
    "axes_test[1,1].set_xlabel('Modelo')\n",
    "axes_test[1,1].set_ylabel('R¬≤')\n",
    "axes_test[1,1].set_title('Comparaci√≥n R¬≤ por Modelo')\n",
    "axes_test[1,1].set_xticks(x_pos)\n",
    "axes_test[1,1].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_test.savefig(f\"{output_dir}ensemble_comparative_analysis_test.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_test)\n",
    "print(f\"üìä Gr√°fico de prueba guardado en: {output_dir}ensemble_comparative_analysis_test.png\")\n",
    "\n",
    "# Visualizaciones para el conjunto de entrenamiento\n",
    "fig_train, axes_train = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes_train[0].scatter(y_train, y_pred_train_opt, alpha=0.6)\n",
    "axes_train[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes_train[0].set_xlabel('Valores Reales')\n",
    "axes_train[0].set_ylabel('Predicciones')\n",
    "axes_train[0].set_title(f'Ensemble Optimizado (Entrenamiento) - R¬≤ = {r2_train_opt:.4f}')\n",
    "axes_train[0].grid(True, alpha=0.3)\n",
    "\n",
    "residuos_train = y_train - y_pred_train_opt\n",
    "axes_train[1].scatter(y_pred_train_opt, residuos_train, alpha=0.6)\n",
    "axes_train[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes_train[1].set_xlabel('Predicciones')\n",
    "axes_train[1].set_ylabel('Residuos')\n",
    "axes_train[1].set_title('Gr√°fico de Residuos (Entrenamiento)')\n",
    "axes_train[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_train.savefig(f\"{output_dir}ensemble_comparative_analysis_train.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_train)\n",
    "print(f\"üìä Gr√°fico de entrenamiento guardado en: {output_dir}ensemble_comparative_analysis_train.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb6eebe",
   "metadata": {},
   "source": [
    "## Resumen de Resultados de Modelos Ensemble\n",
    "La siguiente tabla presenta un resumen comparativo del desempe√±o de diferentes modelos de machine learning utilizados para predecir el consumo energ√©tico (Energy Consumption) en un dataset sint√©tico. Los modelos evaluados incluyen Random Forest, Gradient Boosting, XGBoost (versi√≥n b√°sica), y versiones optimizadas de XGBoost mediante GridSearchCV, RandomizedSearchCV, y BayesSearchCV, junto con los resultados del modelo XGBoost optimizado en los conjuntos de entrenamiento y prueba.\n",
    " \n",
    "## Descripci√≥n de las M√©tricas\n",
    " \n",
    "- Modelo: Nombre del modelo o m√©todo de optimizaci√≥n evaluado.\n",
    "- MSE (Error Cuadr√°tico Medio): Promedio de los errores al cuadrado entre los valores reales y predichos, mide la magnitud de los errores.\n",
    "- RMSE (Ra√≠z del Error Cuadr√°tico Medio): Ra√≠z cuadrada del MSE, proporciona una medida interpretable en la misma unidad que la variable objetivo.\n",
    "- R¬≤ (Coeficiente de Determinaci√≥n): Indica la proporci√≥n de la varianza en la variable objetivo explicada por el modelo (valores cercanos a 1 indican mejor ajuste, ~0.6‚Äì0.9 es realista con datos ruidosos).\n",
    "- MAE (Error Absoluto Medio): Promedio de los errores absolutos, mide la magnitud promedio de los errores sin considerar su direcci√≥n.\n",
    "\n",
    "## Contexto\n",
    "Los datos incluyen ruido a√±adido (y_train_noisy, y_test_noisy) para simular un escenario realista, ya que el dataset original es sint√©tico con una relaci√≥n lineal perfecta. Las m√©tricas reflejan el desempe√±o en el conjunto de prueba (excepto para \"XGBoost Optimizado (Entrenamiento)\"), con valores esperados de R¬≤ ~0.6‚Äì0.9 y RMSE ~400‚Äì600, dependiendo del nivel de ruido.\n",
    "\n",
    "## Uso\n",
    "Esta tabla permite comparar el desempe√±o de los modelos b√°sicos y optimizados, identificando el mejor m√©todo (menor MSE en el conjunto de prueba). Los resultados se guardan en '../data/results/ensemble_resumen_resultados.csv' para an√°lisis posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0adff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMEN DE RESULTADOS ===\n",
      "                               Modelo          MSE      RMSE      R¬≤       MAE\n",
      "0                       Random Forest  232561.3204  482.2461  0.7586  388.5226\n",
      "1                   Gradient Boosting  252435.1228  502.4292  0.7380  397.0668\n",
      "2                             XGBoost  265444.2876  515.2129  0.7245  412.0864\n",
      "3                        GridSearchCV  238829.9856  488.7023  0.7521  393.0759\n",
      "4                  RandomizedSearchCV  242187.9053  492.1259  0.7486  390.9323\n",
      "5                       BayesSearchCV  256503.1255  506.4614  0.7338  404.6343\n",
      "6  XGBoost Optimizado (Entrenamiento)  152717.0443  390.7903  0.8594  312.1552\n",
      "7         XGBoost Optimizado (Prueba)  238829.9856  488.7023  0.7521  393.0759\n",
      "Resumen de resultados guardado en '../data/results/ensemble_resumen_resultados.csv'\n"
     ]
    }
   ],
   "source": [
    "# 9. TABLA RESUMEN DE RESULTADOS\n",
    "print(\"\\n=== RESUMEN DE RESULTADOS ===\")\n",
    "resumen = pd.DataFrame({\n",
    "    'Modelo': ['Random Forest', 'Gradient Boosting', 'XGBoost', 'GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV', 'XGBoost Optimizado (Entrenamiento)', 'XGBoost Optimizado (Prueba)'],\n",
    "    'MSE': [results['Random Forest']['MSE'], results['Gradient Boosting']['MSE'], results['XGBoost']['MSE'], \n",
    "            opt_results['GridSearchCV']['MSE'], opt_results['RandomizedSearchCV']['MSE'], opt_results['BayesSearchCV']['MSE'], \n",
    "            mse_train_opt, mse_test_opt],\n",
    "    'RMSE': [results['Random Forest']['RMSE'], results['Gradient Boosting']['RMSE'], results['XGBoost']['RMSE'], \n",
    "             opt_results['GridSearchCV']['RMSE'], opt_results['RandomizedSearchCV']['RMSE'], opt_results['BayesSearchCV']['RMSE'], \n",
    "             rmse_train_opt, rmse_test_opt],\n",
    "    'R¬≤': [results['Random Forest']['R¬≤'], results['Gradient Boosting']['R¬≤'], results['XGBoost']['R¬≤'], \n",
    "           opt_results['GridSearchCV']['R¬≤'], opt_results['RandomizedSearchCV']['R¬≤'], opt_results['BayesSearchCV']['R¬≤'], \n",
    "           r2_train_opt, r2_test_opt],\n",
    "    'MAE': [results['Random Forest']['MAE'], results['Gradient Boosting']['MAE'], results['XGBoost']['MAE'], \n",
    "            opt_results['GridSearchCV']['MAE'], opt_results['RandomizedSearchCV']['MAE'], opt_results['BayesSearchCV']['MAE'], \n",
    "            mae_train_opt, mae_test_opt]\n",
    "})\n",
    "\n",
    "print(resumen.round(4))\n",
    "\n",
    "# Guardar la tabla resumen como CSV\n",
    "resumen.to_csv('../data/results/ensemble_resumen_resultados.csv', index=False)\n",
    "print(\"Resumen de resultados guardado en '../data/results/ensemble_resumen_resultados.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3f26572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo y scaler guardados como 'ensemble_model.pkl' y 'ensemble_scaler.pkl'\n"
     ]
    }
   ],
   "source": [
    "# 10. GUARDAR EL MODELO\n",
    "joblib.dump(ensemble_optimized, '../data/results/ensemble_model.pkl')\n",
    "joblib.dump(scaler, '../data/results/ensemble_scaler.pkl')\n",
    "print(\"\\nModelo y scaler guardados como 'ensemble_model.pkl' y 'ensemble_scaler.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f658e747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¬°Modelos ensemble implementados exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# 11. FUNCI√ìN PARA NUEVAS PREDICCIONES\n",
    "def predecir_nuevos_datos(nuevos_datos, modelo=ensemble_optimized, escalador=scaler):\n",
    "    \"\"\"\n",
    "    Funci√≥n para hacer predicciones en nuevos datos\n",
    "    \n",
    "    Parameters:\n",
    "    nuevos_datos: array-like o DataFrame, datos a predecir\n",
    "    modelo: modelo ensemble entrenado\n",
    "    escalador: StandardScaler ajustado\n",
    "    \n",
    "    Returns:\n",
    "    predicciones: array con las predicciones\n",
    "    \"\"\"\n",
    "    if isinstance(nuevos_datos, pd.DataFrame):\n",
    "        nuevos_datos = nuevos_datos.values\n",
    "    datos_escalados = escalador.transform(nuevos_datos)\n",
    "    predicciones = modelo.predict(datos_escalados)\n",
    "    return predicciones\n",
    "\n",
    "print(\"\\n¬°Modelos ensemble implementados exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
