{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0128bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import joblib\n",
    "import warnings\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo de gr√°ficos\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f79d1534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. CARGAR Y PREPARAR LOS DATOS\n",
    "train_df = pd.read_csv('../data/processed/energy_data_processed.csv')\n",
    "test_df = pd.read_csv('../data/processed/energy_data_processed_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc71bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del dataset de entrenamiento: (1000, 9)\n",
      "Forma del dataset de prueba: (1000, 9)\n",
      "Variable objetivo: Energy Consumption\n"
     ]
    }
   ],
   "source": [
    "# 2. SEPARAR CARACTER√çSTICAS Y VARIABLE OBJETIVO\n",
    "y_train = train_df['Energy Consumption']\n",
    "X_train = train_df.drop('Energy Consumption', axis=1)\n",
    "y_test = test_df['Energy Consumption']\n",
    "X_test = test_df.drop('Energy Consumption', axis=1)\n",
    "\n",
    "# Verificar tama√±os\n",
    "print(f'Forma del dataset de entrenamiento: {X_train.shape}')\n",
    "print(f'Forma del dataset de prueba: {X_test.shape}')\n",
    "print(f'Variable objetivo: {y_train.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228ebd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ESCALADO DE CARACTER√çSTICAS (opcional para √°rboles, pero √∫til para consistencia)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273364dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Random Forest B√ÅSICO ===\n",
      "MSE: 2097.7550\n",
      "RMSE: 45.8013\n",
      "R¬≤: 0.9976\n",
      "MAE: 36.0567\n",
      "\n",
      "=== Gradient Boosting B√ÅSICO ===\n",
      "MSE: 2536.2069\n",
      "RMSE: 50.3608\n",
      "R¬≤: 0.9971\n",
      "MAE: 39.1547\n",
      "\n",
      "=== XGBoost B√ÅSICO ===\n",
      "MSE: 29.4600\n",
      "RMSE: 5.4277\n",
      "R¬≤: 1.0000\n",
      "MAE: 3.8408\n",
      "\n",
      "XGBoost - Entrenamiento: R¬≤ = 1.0000, MSE = 29.4600\n",
      "XGBoost - Prueba: R¬≤ = 1.0000, MSE = 29.4600\n",
      "\n",
      "Ejemplo de 5 predicciones vs reales (Prueba):\n",
      "Real: 3463.09, Pred: 3463.11, Diferencia: 0.02\n",
      "Real: 5219.66, Pred: 5218.00, Diferencia: 1.66\n",
      "Real: 3106.77, Pred: 3099.42, Diferencia: 7.35\n",
      "Real: 4922.82, Pred: 4917.68, Diferencia: 5.14\n",
      "Real: 4687.67, Pred: 4677.81, Diferencia: 9.86\n",
      "\n",
      "Tama√±os: Train=1000, Test=1000\n",
      "\n",
      "Primeras 5 filas de X_test:\n",
      "   Square Footage  Number of Occupants  Appliances Used  Average Temperature  \\\n",
      "0            7063                   76               10                29.84   \n",
      "1           44372                   66               45                16.72   \n",
      "2           19255                   37               17                14.30   \n",
      "3           13265                   14               41                32.82   \n",
      "4           13375                   26               18                11.92   \n",
      "\n",
      "   Building Type_Commercial  Building Type_Industrial  \\\n",
      "0                     False                     False   \n",
      "1                      True                     False   \n",
      "2                     False                      True   \n",
      "3                     False                     False   \n",
      "4                      True                     False   \n",
      "\n",
      "   Building Type_Residential  Day of Week_Weekday  Day of Week_Weekend  \n",
      "0                       True                 True                False  \n",
      "1                      False                 True                False  \n",
      "2                      False                False                 True  \n",
      "3                       True                 True                False  \n",
      "4                      False                 True                False  \n",
      "\n",
      "Primeras 5 filas de y_test:\n",
      "0    2713.95\n",
      "1    5744.99\n",
      "2    4101.24\n",
      "3    3009.14\n",
      "4    3279.17\n",
      "Name: Energy Consumption, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 4. MODELOS ENSEMBLE\n",
    "models = {\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10, min_samples_split=5, min_samples_leaf=2),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=5, min_samples_split=5, min_samples_leaf=2),\n",
    "    'XGBoost': XGBRegressor(n_estimators=50, random_state=42, eval_metric='rmse', max_depth=3, reg_alpha=1.0, reg_lambda=2.0, gamma=0.5)\n",
    "}\n",
    "\n",
    "# Entrenar y evaluar modelos b√°sicos\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f'\\n=== {name} B√ÅSICO ===')\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    results[name] = {'MSE': mse, 'RMSE': rmse, 'R¬≤': r2, 'MAE': mae}\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Diagn√≥stico para XGBoost\n",
    "# Agregar despu√©s del bucle for en la celda 3 (despu√©s de entrenar los modelos b√°sicos)\n",
    "y_pred_train_xgb = models['XGBoost'].predict(X_train_scaled)\n",
    "mse_train_xgb = mean_squared_error(y_train, y_pred_train_xgb)\n",
    "r2_train_xgb = r2_score(y_train, y_pred_train_xgb)\n",
    "print(f\"\\nXGBoost - Entrenamiento: R¬≤ = {r2_train_xgb:.4f}, MSE = {mse_train_xgb:.4f}\")\n",
    "print(f\"XGBoost - Prueba: R¬≤ = {results['XGBoost']['R¬≤']:.4f}, MSE = {results['XGBoost']['MSE']:.4f}\")\n",
    "\n",
    "# Ejemplo de 5 predicciones vs reales en prueba\n",
    "y_pred_test_xgb = models['XGBoost'].predict(X_test_scaled)\n",
    "print(\"\\nEjemplo de 5 predicciones vs reales (Prueba):\")\n",
    "sample_idx = np.random.choice(len(y_test), min(5, len(y_test)), replace=False)\n",
    "for i in sample_idx:\n",
    "    print(f\"Real: {y_test.iloc[i]:.2f}, Pred: {y_pred_test_xgb[i]:.2f}, Diferencia: {abs(y_test.iloc[i] - y_pred_test_xgb[i]):.2f}\")\n",
    "# Tama√±o de datasets\n",
    "print(f\"\\nTama√±os: Train={len(y_train)}, Test={len(y_test)}\")\n",
    "\n",
    "# Inspecci√≥n de datos\n",
    "print(\"\\nPrimeras 5 filas de X_test:\")\n",
    "print(X_test.head())\n",
    "print(\"\\nPrimeras 5 filas de y_test:\")\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6f34b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¬°SVR implementado exitosamente!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DIAGN√ìSTICO ADICIONAL ===\")\n",
    "\n",
    "# Correlaciones entre caracter√≠sticas y variable objetivo\n",
    "correlations = X_train.corrwith(y_train)\n",
    "print(\"\\nCorrelaciones entre caracter√≠sticas y 'Energy Consumption':\")\n",
    "print(correlations.sort_values(ascending=False))\n",
    "\n",
    "# Validaci√≥n cruzada para XGBoost b√°sico\n",
    "cv_scores = cross_val_score(models['XGBoost'], X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "print(f\"\\nValidaci√≥n cruzada (5-fold) para XGBoost - R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Comparar primeras filas de X_train y X_test\n",
    "print(\"\\nPrimeras 5 filas de X_train:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Mejores par√°metros para XGBoost: OrderedDict({'colsample_bytree': 0.8155553746682142, 'learning_rate': 0.15943036309640915, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.6})\n",
      "Mejor score CV: 5645.3567\n"
     ]
    }
   ],
   "source": [
    "# 5. OPTIMIZACI√ìN CON BAYESIAN SEARCH PARA XGBoost (ejemplo, puedes extender a otros)\n",
    "# Optimizaci√≥n de hiperpar√°metros para XGBoost\n",
    "print(\"\\n=== OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS PARA XGBOOST ===\")\n",
    "\n",
    "# Definir espacio de hiperpar√°metros com√∫n\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'reg_alpha': [0.0, 0.5, 1.0],\n",
    "    'reg_lambda': [0.0, 1.0, 2.0],\n",
    "    'gamma': [0.0, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "# 1. GridSearchCV\n",
    "print(\"\\n--- GridSearchCV ---\")\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    param_grid={\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.1, 0.3],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0.0, 1.0],\n",
    "        'reg_lambda': [1.0, 2.0]\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "grid_best = grid_search.best_estimator_\n",
    "grid_score = -grid_search.best_score_\n",
    "print(f\"Mejores par√°metros (GridSearchCV): {grid_search.best_params_}\")\n",
    "print(f\"Mejor MSE CV (GridSearchCV): {grid_score:.4f}\")\n",
    "\n",
    "# 2. RandomizedSearchCV\n",
    "print(\"\\n--- RandomizedSearchCV ---\")\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "random_best = random_search.best_estimator_\n",
    "random_score = -random_search.best_score_\n",
    "print(f\"Mejores par√°metros (RandomizedSearchCV): {random_search.best_params_}\")\n",
    "print(f\"Mejor MSE CV (RandomizedSearchCV): {random_score:.4f}\")\n",
    "\n",
    "# 3. BayesSearchCV\n",
    "print(\"\\n--- BayesSearchCV ---\")\n",
    "bayes_search = BayesSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42, eval_metric='rmse'),\n",
    "    search_spaces={\n",
    "        'n_estimators': Integer(50, 200),\n",
    "        'max_depth': Integer(3, 8),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0),\n",
    "        'reg_alpha': Real(0.0, 1.0),\n",
    "        'reg_lambda': Real(0.0, 2.0),\n",
    "        'gamma': Real(0.0, 0.5)\n",
    "    },\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "# Agregar early stopping\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)\n",
    "bayes_search.fit(X_train_split, y_train_split, early_stopping_rounds=10, eval_set=[(X_val_split, y_val_split)], verbose=False)\n",
    "bayes_best = bayes_search.best_estimator_\n",
    "bayes_score = -bayes_search.best_score_\n",
    "print(f\"Mejores par√°metros (BayesSearchCV): {bayes_search.best_params_}\")\n",
    "print(f\"Mejor MSE CV (BayesSearchCV): {bayes_score:.4f}\")\n",
    "\n",
    "# Comparar resultados\n",
    "print(\"\\n=== COMPARACI√ìN DE M√âTODOS DE OPTIMIZACI√ìN ===\")\n",
    "opt_results = {}\n",
    "for name, model in [('GridSearchCV', grid_best), ('RandomizedSearchCV', random_best), ('BayesSearchCV', bayes_best)]:\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    opt_results[name] = {'MSE': mse, 'RMSE': rmse, 'R¬≤': r2, 'MAE': mae}\n",
    "    print(f\"\\n{name} (Prueba):\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R¬≤: {r2:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# Seleccionar el mejor modelo\n",
    "best_method = min(opt_results, key=lambda x: opt_results[x]['MSE'])\n",
    "ensemble_optimized = {'GridSearchCV': grid_best, 'RandomizedSearchCV': random_best, 'BayesSearchCV': bayes_best}[best_method]\n",
    "print(f\"\\nMejor m√©todo: {best_method} (MSE: {opt_results[best_method]['MSE']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c507bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODELO OPTIMIZADO (ENTRENAMIENTO) ===\n",
      "MSE: 1246.6236\n",
      "RMSE: 35.3076\n",
      "R¬≤: 0.9986\n",
      "MAE: 27.7898\n",
      "\n",
      "=== MODELO OPTIMIZADO (PRUEBA) ===\n",
      "MSE: 1246.6236\n",
      "RMSE: 35.3076\n",
      "R¬≤: 0.9986\n",
      "MAE: 27.7898\n",
      "\n",
      "CV RMSE: 67.4988 (+/- 36.0738)\n"
     ]
    }
   ],
   "source": [
    "# 6. MODELO OPTIMIZADO (usamos XGBoost optimizado como ejemplo principal)\n",
    "y_pred_train_opt = ensemble_optimized.predict(X_train_scaled)\n",
    "y_pred_test_opt = ensemble_optimized.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas para entrenamiento\n",
    "mse_train_opt = mean_squared_error(y_train, y_pred_train_opt)\n",
    "rmse_train_opt = np.sqrt(mse_train_opt)\n",
    "r2_train_opt = r2_score(y_train, y_pred_train_opt)\n",
    "mae_train_opt = mean_absolute_error(y_train, y_pred_train_opt)\n",
    "\n",
    "# M√©tricas para prueba\n",
    "mse_test_opt = mean_squared_error(y_test, y_pred_test_opt)\n",
    "rmse_test_opt = np.sqrt(mse_test_opt)\n",
    "r2_test_opt = r2_score(y_test, y_pred_test_opt)\n",
    "mae_test_opt = mean_absolute_error(y_test, y_pred_test_opt)\n",
    "\n",
    "print(f'\\n=== MODELO OPTIMIZADO (ENTRENAMIENTO) ===')\n",
    "print(f'MSE: {mse_train_opt:.4f}')\n",
    "print(f'RMSE: {rmse_train_opt:.4f}')\n",
    "print(f'R¬≤: {r2_train_opt:.4f}')\n",
    "print(f'MAE: {mae_train_opt:.4f}')\n",
    "\n",
    "print(f'\\n=== MODELO OPTIMIZADO (PRUEBA) ===')\n",
    "print(f'MSE: {mse_test_opt:.4f}')\n",
    "print(f'RMSE: {rmse_test_opt:.4f}')\n",
    "print(f'R¬≤: {r2_test_opt:.4f}')\n",
    "print(f'MAE: {mae_test_opt:.4f}')\n",
    "\n",
    "# Validaci√≥n cruzada\n",
    "cv_scores = cross_val_score(ensemble_optimized, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f'\\nCV RMSE: {np.sqrt(-cv_scores.mean()):.4f} (+/- {np.sqrt(cv_scores.std() * 2):.4f})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4666f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicciones de entrenamiento guardadas en '../data/results/ensemble_predictions_train.csv'\n",
      "Predicciones de prueba guardadas en '../data/results/ensemble_predictions_test.csv'\n"
     ]
    }
   ],
   "source": [
    "# 7. GUARDAR RESULTADOS EN CSV\n",
    "train_results = pd.DataFrame({\n",
    "    'Valores Reales': y_train,\n",
    "    'Predicciones': y_pred_train_opt,\n",
    "    'Diferencia': y_train - y_pred_train_opt\n",
    "})\n",
    "train_results.to_csv('../data/results/ensemble_predictions_train.csv', index=False)\n",
    "print(\"\\nPredicciones de entrenamiento guardadas en '../data/results/ensemble_predictions_train.csv'\")\n",
    "\n",
    "test_results = pd.DataFrame({\n",
    "    'Valores Reales': y_test,\n",
    "    'Predicciones': y_pred_test_opt,\n",
    "    'Diferencia': y_test - y_pred_test_opt\n",
    "})\n",
    "test_results.to_csv('../data/results/ensemble_predictions_test.csv', index=False)\n",
    "print(\"Predicciones de prueba guardadas en '../data/results/ensemble_predictions_test.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462dd49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Gr√°fico de prueba guardado en: ../data/figures/ensemble_comparative_analysis_test.png\n",
      "üìä Gr√°fico de entrenamiento guardado en: ../data/figures/ensemble_comparative_analysis_train.png\n"
     ]
    }
   ],
   "source": [
    "# 8. VISUALIZACIONES\n",
    "output_dir = \"../data/figures/\"\n",
    "\n",
    "# Visualizaciones para el conjunto de prueba\n",
    "fig_test, axes_test = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "axes_test[0,0].scatter(y_test, y_pred_test_opt, alpha=0.6)\n",
    "axes_test[0,0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes_test[0,0].set_xlabel('Valores Reales')\n",
    "axes_test[0,0].set_ylabel('Predicciones')\n",
    "axes_test[0,0].set_title(f'Ensemble Optimizado (Prueba) - R¬≤ = {r2_test_opt:.4f}')\n",
    "axes_test[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "residuos_test = y_test - y_pred_test_opt\n",
    "axes_test[0,1].scatter(y_pred_test_opt, residuos_test, alpha=0.6)\n",
    "axes_test[0,1].axhline(y=0, color='r', linestyle='--')\n",
    "axes_test[0,1].set_xlabel('Predicciones')\n",
    "axes_test[0,1].set_ylabel('Residuos')\n",
    "axes_test[0,1].set_title('Gr√°fico de Residuos (Prueba)')\n",
    "axes_test[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "model_names = list(results.keys()) + ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']\n",
    "mse_values = [results[m]['MSE'] for m in results] + [opt_results[m]['MSE'] for m in ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']]\n",
    "r2_values = [results[m]['R¬≤'] for m in results] + [opt_results[m]['R¬≤'] for m in ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']]\n",
    "\n",
    "x_pos = np.arange(len(model_names))\n",
    "axes_test[1,0].bar(x_pos, mse_values, alpha=0.7)\n",
    "axes_test[1,0].set_xlabel('Modelo')\n",
    "axes_test[1,0].set_ylabel('MSE')\n",
    "axes_test[1,0].set_title('Comparaci√≥n MSE por Modelo')\n",
    "axes_test[1,0].set_xticks(x_pos)\n",
    "axes_test[1,0].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "axes_test[1,1].bar(x_pos, r2_values, alpha=0.7, color='green')\n",
    "axes_test[1,1].set_xlabel('Modelo')\n",
    "axes_test[1,1].set_ylabel('R¬≤')\n",
    "axes_test[1,1].set_title('Comparaci√≥n R¬≤ por Modelo')\n",
    "axes_test[1,1].set_xticks(x_pos)\n",
    "axes_test[1,1].set_xticklabels(model_names, rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_test.savefig(f\"{output_dir}ensemble_comparative_analysis_test.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_test)\n",
    "print(f\"üìä Gr√°fico de prueba guardado en: {output_dir}ensemble_comparative_analysis_test.png\")\n",
    "\n",
    "# Visualizaciones para el conjunto de entrenamiento\n",
    "fig_train, axes_train = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "axes_train[0].scatter(y_train, y_pred_train_opt, alpha=0.6)\n",
    "axes_train[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes_train[0].set_xlabel('Valores Reales')\n",
    "axes_train[0].set_ylabel('Predicciones')\n",
    "axes_train[0].set_title(f'Ensemble Optimizado (Entrenamiento) - R¬≤ = {r2_train_opt:.4f}')\n",
    "axes_train[0].grid(True, alpha=0.3)\n",
    "\n",
    "residuos_train = y_train - y_pred_train_opt\n",
    "axes_train[1].scatter(y_pred_train_opt, residuos_train, alpha=0.6)\n",
    "axes_train[1].axhline(y=0, color='r', linestyle='--')\n",
    "axes_train[1].set_xlabel('Predicciones')\n",
    "axes_train[1].set_ylabel('Residuos')\n",
    "axes_train[1].set_title('Gr√°fico de Residuos (Entrenamiento)')\n",
    "axes_train[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig_train.savefig(f\"{output_dir}ensemble_comparative_analysis_train.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close(fig_train)\n",
    "print(f\"üìä Gr√°fico de entrenamiento guardado en: {output_dir}ensemble_comparative_analysis_train.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da75ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMEN DE RESULTADOS ===\n",
      "                              Modelo        MSE     RMSE      R¬≤      MAE\n",
      "Random Forest          Random Forest  2097.7550  45.8013  0.9976  36.0567\n",
      "Gradient Boosting  Gradient Boosting  2536.2069  50.3608  0.9971  39.1547\n",
      "XGBoost                      XGBoost    29.4600   5.4277  1.0000   3.8408\n"
     ]
    }
   ],
   "source": [
    "# 9. TABLA RESUMEN DE RESULTADOS\n",
    "print(\"\\n=== RESUMEN DE RESULTADOS ===\")\n",
    "resumen = pd.DataFrame({\n",
    "    'Modelo': ['Random Forest', 'Gradient Boosting', 'XGBoost', 'GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV', 'XGBoost Optimizado (Entrenamiento)', 'XGBoost Optimizado (Prueba)'],\n",
    "    'MSE': [results['Random Forest']['MSE'], results['Gradient Boosting']['MSE'], results['XGBoost']['MSE'], \n",
    "            opt_results['GridSearchCV']['MSE'], opt_results['RandomizedSearchCV']['MSE'], opt_results['BayesSearchCV']['MSE'], \n",
    "            mse_train_opt, mse_test_opt],\n",
    "    'RMSE': [results['Random Forest']['RMSE'], results['Gradient Boosting']['RMSE'], results['XGBoost']['RMSE'], \n",
    "             opt_results['GridSearchCV']['RMSE'], opt_results['RandomizedSearchCV']['RMSE'], opt_results['BayesSearchCV']['RMSE'], \n",
    "             rmse_train_opt, rmse_test_opt],\n",
    "    'R¬≤': [results['Random Forest']['R¬≤'], results['Gradient Boosting']['R¬≤'], results['XGBoost']['R¬≤'], \n",
    "           opt_results['GridSearchCV']['R¬≤'], opt_results['RandomizedSearchCV']['R¬≤'], opt_results['BayesSearchCV']['R¬≤'], \n",
    "           r2_train_opt, r2_test_opt],\n",
    "    'MAE': [results['Random Forest']['MAE'], results['Gradient Boosting']['MAE'], results['XGBoost']['MAE'], \n",
    "            opt_results['GridSearchCV']['MAE'], opt_results['RandomizedSearchCV']['MAE'], opt_results['BayesSearchCV']['MAE'], \n",
    "            mae_train_opt, mae_test_opt]\n",
    "})\n",
    "print(resumen.round(4))\n",
    "\n",
    "# Guardar la tabla resumen como CSV\n",
    "resumen.to_csv('../data/results/ensemble_resumen_resultados.csv', index=False)\n",
    "print(\"Resumen de resultados guardado en '../data/results/ensemble_resumen_resultados.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffc767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Modelo y scaler guardados como 'ensemble_model.pkl' y 'ensemble_scaler.pkl'\n"
     ]
    }
   ],
   "source": [
    "# 10. GUARDAR EL MODELO\n",
    "joblib.dump(ensemble_optimized, '../data/results/ensemble_model.pkl')\n",
    "joblib.dump(scaler, '../data/results/ensemble_scaler.pkl')\n",
    "print(\"\\nModelo y scaler guardados como 'ensemble_model.pkl' y 'ensemble_scaler.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba924d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¬°Modelos ensemble implementados exitosamente!\n"
     ]
    }
   ],
   "source": [
    "# 11. FUNCI√ìN PARA NUEVAS PREDICCIONES\n",
    "def predecir_nuevos_datos(nuevos_datos, modelo=ensemble_optimized, escalador=scaler):\n",
    "    \"\"\"\n",
    "    Funci√≥n para hacer predicciones en nuevos datos\n",
    "    \n",
    "    Parameters:\n",
    "    nuevos_datos: array-like o DataFrame, datos a predecir\n",
    "    modelo: modelo ensemble entrenado\n",
    "    escalador: StandardScaler ajustado\n",
    "    \n",
    "    Returns:\n",
    "    predicciones: array con las predicciones\n",
    "    \"\"\"\n",
    "    if isinstance(nuevos_datos, pd.DataFrame):\n",
    "        nuevos_datos = nuevos_datos.values\n",
    "    datos_escalados = escalador.transform(nuevos_datos)\n",
    "    predicciones = modelo.predict(datos_escalados)\n",
    "    return predicciones\n",
    "\n",
    "print(\"\\n¬°Modelos ensemble implementados exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
